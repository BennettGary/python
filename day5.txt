day5
# 第二章 获取文本语料和词汇资源
# 2.1获取文本语料库

1.古腾堡语料库
  NLTK包含古腾堡项目（Project Gutenberg）
  下面是这个语料库中文件标识符
    >>> import nltk
    >>> nltk.corpus.gutenberg.fileids()
    ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt',
    'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 
    'chesterton-brown.txt','chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 
    'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 
    'whitman-leaves.txt']
  其中包含有很多的文本，我们选择其中一个进行操作。
    >>> emma = nltk.corpus.gutenberg.words('austen-emma.txt')
    >>> len(emma)
    192427
  我们还可以使用之前学习的一些文本操作方法来对导入的语料库进行操作
  例如：
    >>> emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))
    >>> emma.concordance('surprize')
	Displaying 25 of 37 matches:
	er father , was sometimes taken by surprize at his being still able to pity...
  我们在定义emma时，我们调用了NLTK中的corpus包中的gutenberg对象的words()函数。
  
  但因为我们总是要输入这么长的名字很繁琐，Python提供了另一个版本的import语句，示例如下：
    >>> from nltk.corpus import gutenberg
    >>> gutenberg.fileids()
    ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt',...]
    >>> emma = gutenberg.words('austen-emma.txt')
  导入语料库之后，让我们写一个简单的程序，通过循环遍历前面列出的gutenberg文件标识符链表相应的fileid，然后计算统计每个文本。
    >>>  for fileid in gutenberg.fileids():
         	num_chars = len(gutenberg.raw(fileid))
         	num_words = len(gutenberg.words(fileid))
         	num_sents = len(gutenberg.sents(fileid))
	 	num_vocab = len(set([word.lower() for word in gutenberg.words(fileid)]))
	 	print(num_chars,num_words,num_sents,num_vocab,fileid)
   
    887071 192427 7752 7344 austen-emma.txt
    466292 98171 3747 5835 austen-persuasion.txt
    673022 141576 4999 6403 austen-sense.txt
    4332554 1010654 30103 12767 bible-kjv.txt
    38153 8354 438 1535 blake-poems.txt
    249439 55563 2863 3940 bryant-stories.txt
    84663 18963 1054 1559 burgess-busterbrown.txt
    144395 34110 1703 2636 carroll-alice.txt
    457450 96996 4779 8335 chesterton-ball.txt
    406629 86063 3806 7794 chesterton-brown.txt
    320525 69213 3742 6349 chesterton-thursday.txt
    935158 210663 10230 8447 edgeworth-parents.txt
    1242990 260819 10059 17231 melville-moby_dick.txt
    468220 96825 1851 9021 milton-paradise.txt
    112310 25833 2163 3032 shakespeare-caesar.txt
    162881 37360 3106 4716 shakespeare-hamlet.txt
    100351 23140 1907 3464 shakespeare-macbeth.txt
    711215 154883 4250 12452 whitman-leaves.txt

    raw()函数给我们没有进行过任何语言学处理的文件的内容。因此，例如：len(gutenberg.raw('blake-poems.txt')告诉我们文本中出现的词汇个数，包括词之间的
空格。sents()函数把文本划分为句子，其中每个句子是一个词链表。
      >>> macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')
      >>> macbeth_sentences
      [['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']'], ['Actus', 'Primus', '.'], ...]
      >>> macbeth_sentences[1037]
      ['Good', 'night', ',', 'and', 'better', 'health', 'Attend', 'his', 'Maiesty']
      >>> longest_len = max([len(s) for s in macbeth_sentences])
      >>> [s for s in macbeth_sentences if len(s) == longest_len]
      [['Doubtfull', 'it', 'stood', ',', 'As', 'two', 'spent', 'Swimmers', ',', 'that', 'doe', 'cling', 'together', ',', 'And', 'choake', 'their', 'Art', ':', 'The', 'mercilesse...]]

2.网络和聊天文本
    虽然gutenberg项目中包含成千上万的书籍，它代表既定的文学。考虑到较不正式的语言也是很重要的。
    所以我们导入NLTK的网络文本小集合
      >>> from nltk.corpus import webtext
      >>> for fileid in webtext.fileids():
          	print(fileid)
      firefox.txt
      grail.txt
      overheard.txt
      pirates.txt
      singles.txt
      wine.txt
    还有一个即时聊天会话语料库
    >>> from nltk.corpus import nps_chat
    >>> chatroom = nps_chat.posts('10-19-20s_706posts.xml')
    >>> chatroom[123]
    ['i', 'do', "n't", 'want', 'hot', 'pics', 'of', 'a', 'female', ',', 'I', 'can', 'look', 'in', 'a', 'mirror', '.']

3.布朗语料库
    >>> from nltk.corpus import brown
    >>> brown.categories()
    ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore','mystery','news','religion', 'reviews', 'romance', 'science_fiction']
    >>> brown.words(categories= 'news')
    ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]
    >>> brown.words(fileids=['cg22'])
    ['Does', 'our', 'society', 'have', 'a', 'runaway', ',', ...]
    >>> brown.sents(categories=['news','editorial','reviews'])
    [['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', "Atlanta's", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', "''", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', "''", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]
    布朗语料库是一个研究文体之间的系统性差异————一种叫做文体学的语言学研究————很方便的资源。
    让我们来比较不同的文体中的情态动词的用法。
    第一步：产生特定文本的计数，记住做下面的实验之前要import nltk
    >>> from nltk.corpus import brown
    >>> news_text = brown.words(categories='news')
    >>> fdist = nltk.FreqDist([w.lower() for w in news_text])
    >>> modals = ['can','could','may','might','must','will']
    >>> for m in modals:
	    print(m + ':',fdist[m])
	    
	can: 94  could: 87  may: 93  might: 38  must: 53  will: 389


    下面，我们来统计每一个文体中的情态动词
    >>> cfd = nltk.ConditionalFreqDist(
    		(genre,word)
		for genre in brown.catagories()
		for word in brown.words(catagories=genre))
    >>> genres = ['news','religion','hobbies','science_fiction','romance','humor']
    >>> modals = ['can','could','may','might','must','will']
    >>> cfd.tabulate(conditions=genres, samples=modals)
                  can could   may might  must  will 
           news    93    86    66    38    50   389 
       religion    82    59    78    12    54    71 
        hobbies   268    58   131    22    83   264 
science_fiction    16    49     4    12     8    16 
        romance    74   193    11    51    45    43 
          humor    16    30     8     8     9    13 


4.路透社语料库
    包含了10788个新闻文档，共计130万字。这些文档分为90个主题，按照“训练”和“测试”分为两组。
    >>> from nltk.corpus import reuters
    >>> reuters.fileids()
    ['training/7406', 'training/7407', 'training/7408', 'training/741', 'training/7410', 'training/7413', 'training/7419', 'training/742'...]
    >>> reuters.categories()
    ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', ...]
    
    语料库方法既接受单个的fileid也接受fileids列表作为参数
    >>> reuters.categories('training/9865')
    ['barley', 'corn', 'grain', 'wheat']
    >>> reuters.categories(['training/9865','training/9880'])
    ['barley', 'corn', 'grain', 'money-fx', 'wheat']
    >>> reuters.fileids('barley')
    ['test/15618', 'test/15649', 'test/15676', 'test/15728', 'test/15871', 'test/15875', 'test/15952', 'test/17767', 'test/17769', 'test/18024', ...]
    >>> reuters.fileids(['barley','corn'])
    ['test/14832', 'test/14858', 'test/15033', 'test/15043', 'test/15106', 'test/15287', 'test/15341', 'test/15618', 'test/15648', 'test/15649', 'test/15676', 'test/15686', 'test/15720', 'test/15728', 'test/15845', 'test/15856', 'test/15860',...]
    
    类似的我们可以以文档或类别为单位查找我们想要的词或句子
    >>> reuters.words('training/9865')[:14]
    >>> reuters.words(categories='barley')
    
5.就职演讲语料库
    >>> from nltk.corpus import inaugural
    >>> inaugural.fileids()
    >>> [fileid[:4] for fileid in inaugural.fileids()]
    注：每个文本的年代都出现在它的文件名中。要从文件名中提取出年代，我们使用fileid[:4]提取前四个字符
    
    >>> cfd = nltk.ConditionalFreqDist(
    		(target,fileid[:4])
		for fileid in inaugural.fileids()
		for w in inaugural.words(fileid)
		for target in ['america','citizen']
		if w.lower().startswith(target))
    >>> cfd.plot()
    注：产生的条件频率分布图，计数就职演讲语料库中所有以america或citizen开始的词。每个演讲单独计数。这样就能观察出随时间变化用法上的演讲趋势，计数没有与文档长度进行归一化处理
    
    
6.其他语言的语料库
    nltk中包含多国语言语料库。某种情况下你在使用这些语料库之前需要学习如何在python中处理字符编码。（后面会详细讲述）
    >>> nltk.corpus.cess_esp.words()
    >>> nltk.corpus.floresta.words()
    >>> nltk.corpus.indian.words('hindi.pos')
    >>> nltk.corpus.udhr.fileids()
    >>> nltk.corpus.udhr.words('Javanese-Latin1')[11:]
    这些语料库的最后，udhr，是超过300中语言的世界人权宣言。这个语料库的fileids包括有关文件所使用的字符编码，如utf-8或者是Latin1。
    下面我们用条件频率分布来研究"世界人权宣言"（udhr）语料库中不同语言版本中字长的差异。
    
    >>> from nltk.corpus import udhr
    >>> languages = ['Chickasaw','English','German_Deutsch','Greenlandic_Inuktikut','Hungarian_Magyar','Ibibio_Efik']
    >>> cfd = nltk.ConditionalFreqDist(
    		(lang,len(word))
		for lang in languages
		for word in udhr.words(lang + '-Latin1'))
    >>> cfd.plot(cumulative = True)
    
7.文本语料库的结构
    常见结构：
        最简单的一种语料库是一些孤立的没有什么特别的组织的文本集合
        一些语料库如文体（布朗语料库）等分类组织结构
	一些分类会重叠，如主题类别（路透社语料库）
	另外一些语料库可以表示随时间变化语言用法的改变（就职演说语料库）
	
NLTK中定义的基本语料库函数：使用help(nltk.corpus.reader)可以找到更多的文档
    fileids()		                                语料库中的文件			
    fileids([categories])                               这些分类所对应的语料库中的文件
    categories()                                        语料库中的分类
    categories([fileids])                               这些文件对应的语料库中的分类
    raw()                                               语料库的原始内容
    raw(fileids = [f1,f2,f3])                           指定文件的原始内容
    raw(categories = [c1,c2])                           指定分类的原始内容
    words()                                             整个语料库中的词汇
    words(fileids = [f1,f2,f3])                         指定文件中的词汇 
    words(categories = [c1,c2])                         制定分类中的词汇
    sents()                                             整个分类中的句子
    sents(fileids = [f1,f2,f3])                         指定文件中的句子
    sents(categories = [c1,c2])                         指定分类中的句子
    abspath(fileid)                                     指定文件在磁盘中的位置
    encoding(fileid)                                    文件的编码（如果知道的话）
    open(fileid)                                        打开指定语料库文件的文件流
    root()                                              到本地安装的语料库根目录的路径

